---
layout: post
title: IDA and EDA, what is this?
subtitle: Understanding the conceptual frameworks of IDA and EDA
thumbnail-img: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQrTyMxUic222An0o3kZeGvy6PGFNACLEsxig&usqp=CAU
#share-img: 
gh-repo: renan2scarvalho/
gh-badge: [star, fork, follow]
tags: [data science, data analysis, statistics]
comments: true
---

Nowadays, data is ubiquitous, hence, is necessary analyze it with scrutiny. In this context, a data analysist (conceptual context, not enterprise *fix-it* employee) work would be analyze the data, and just that, and this is what we'll talk about here. And by analyzing data I mean find what that that is about, find missing values, find trends, and finally, after all these steps, support decision makers, or in other words, business. So let's now approach our subjects.

While **IDA** stands for **Initial Data Analysis**, **EDA** stands for **Exploratory Data Analysis**. 
Their names look similar, but what's the main difference between them? What are they used for? Our target on this post is to address this difference, 
understanding clearly the *conceptual framework* for both analysis.
So let's *check it out!*

## 1. IDA

*Initial Data Analysis (IDA)* are all steps that are took *after data collection* and *prior to any formal statistic analysis*, which aims to answer the research question. So ideally, IDA should be carried alongside data collections, aiming to detect and handle issues as soon as possible [[1]](https://cstat.msu.edu/feature/initial-data-analysis). A large amount of time is usually allocated to statistical analysis, such as 80%, time spent with data cleaning and preparation. But carrying out these steps in an informal or unstructured way - i.e. neglecting or disorganized, may result in large and non-transparent impact on conclusions. So this is the main point of IDA, while targeting the research question, document changes to keep transparency [[2]](https://reader.elsevier.com/reader/sd/pii/S0022522315017948?token=07FBCC432E99AD4028F1B28789FBBECDEF2E6EE063BEFBC9A765FF4E7974C6127C697422D6663B03E56BD272033E696C). Examples are information gathered after IDA data cleaning, exclusion of cases, unusual observation, creating of new or transformed variables, distributions, or missing data patterns [[3]](https://obsstudies.org/wp-content/uploads/2018/04/idarev2.pdf). \
In a conceptual framework, IDA can be thought as a process of data inspection divided into some steps, where some authors divide into three, while others divide into six steps. Here we will show the *six steps* path as shown in [[3]](https://obsstudies.org/wp-content/uploads/2018/04/idarev2.pdf), as in the Figure above:

![image](https://user-images.githubusercontent.com/63553829/94266078-fa784600-ff0f-11ea-896f-222cfc91b213.png){: .align-center}

### 1.1. Metadata setup

Sometimes the dataset alone isn't sufficient to do a meaningful analysis, so background information is needed, or, in other words, *metadata* - "structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use, or manage an information". Here we have **technical metadata**, such as variables and values, data type, measurement unit, plausability lmits or codes for missing values, essentail for IDA data cleaning and data screening; and **conceptual data**, which relates to the background of data collection and its implementation, regarding design (data sources and collection methods), recruitment (target population, sampling methods, time of data collection), or study (validation status of instruments, training of examiners).\
Metadata of relevance msut be added in a systematic way so that one can cover all elements of interest. A good practice is to set up all relevant metadata into an electronic form such as databases (e.g. MySQL) before starting the study.

### 1.2. Data cleaning

As the name says, here data is processed in order to arrive at a cleaned dataset in the needed format for the statistical analysis. This may include several different steps such as creating of new variables, splitting or merging datasets, and must be performed in a systematic way to find data errors (which are highly variable) and, if possible, correct them.\
This step can be performed by inspection with the use of simple techniques such as tabulations, cross tabulations, histograms, or scatter plots. some best practices are using well established coding schemes or formats such as for date and time.

### 1.3. Data screening

Data screening tryies to understand properties of data that may affect future analysis, or, in other words, is an attempt to detect structural errors which points  to several different problems (e.g. recruitment process, choice of instruments, etc.), which result in a deviation between the expected and observed data. A simple example consists in imbalanced data, so let's imagine one wants to assess risk level. In this case one must include subjects with low, medium, and high risk profile. But if for some reason the number of subjects on high risk are rare (extremely low number of subjects regarding the others - in-class imbalance), this study would result in only limited conclusions. The aim here is hence variable, so let's brief it:

- *distribution of single variables:* identify qualitative properties (outliers, bimodality, skewness) and compare characteristics through visual inspection (boxplots,histograms, bar charts, etc.)
- *missing data:* methods to handle missing data, with their types, level of occurence, etc.
- *association between variables:* comparison, visualization and quantification of pairwise associations, and developing ideas for summary variables to reduce dimensionality

### 1.4. Documentation and initial data reporting

The previous steps are laborious and a large amount of information is created. So in order to avoid losing information over these steps, an initial data report is quite useful and necessary. Here we have another briefing of what should be put in that report:\
1. A short summary of the metadata set up step, focusing on the sources of metadata, and a summary of the research questions and study design.
2. A detailed study flowchart, starting with all criteria defining population and recruitment process, number of subjects before and after inclusion/exclusion criteria, impossible values, missing data, etc.
3. A short summary of data processing and cleaning step, with all correction rules and impact of changes (e.g. proportion of elements when dealing with imbalanced data).
4. 




Some people classify IDA as only assessing for missing values, which now can be clearly viewed as an oversimplification, once the process, as shown, is broader.


## EDA














