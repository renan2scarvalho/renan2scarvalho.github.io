---
layout: post
title: IDA and EDA, what is this?
subtitle: Understanding the conceptual frameworks of IDA and EDA
thumbnail-img: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQrTyMxUic222An0o3kZeGvy6PGFNACLEsxig&usqp=CAU
#share-img: 
gh-repo: renan2scarvalho/
gh-badge: [star, fork, follow]
tags: [data science, data analysis, statistics]
comments: true
---

Nowadays, data is ubiquitous, hence, is necessary analyze it with scrutiny. In this context, a data analysist (conceptual context, not enterprise *fix-it* employee) work would be analyze the data, and just that, and this is what we'll talk about here. And by analyzing data I mean find what that that is about, find missing values, find trends, and finally, after all these steps, support decision makers, or in other words, business. So let's now approach our subjects.

While **IDA** stands for **Initial Data Analysis**, **EDA** stands for **Exploratory Data Analysis**. 
Their names look similar, but what's the main difference between them? What are they used for? Our target on this post is to address this difference, 
understanding clearly the *conceptual framework* for both analysis.
So let's *check it out!*

## 1. IDA

*Initial Data Analysis (IDA)* are all steps that are took *after data collection* and *prior to any formal statistic analysis*, which aims to answer the research question. So ideally, IDA should be carried alongside data collections, aiming to detect and handle issues as soon as possible [[1]](https://cstat.msu.edu/feature/initial-data-analysis). A large amount of time is usually allocated to statistical analysis, such as 80%, time spent with data cleaning and preparation. But carrying out these steps in an informal or unstructured way - i.e. neglecting or disorganized, may result in large and non-transparent impact on conclusions. So this is the main point of IDA, while targeting the research question, document changes to keep transparency [[2]](https://reader.elsevier.com/reader/sd/pii/S0022522315017948?token=07FBCC432E99AD4028F1B28789FBBECDEF2E6EE063BEFBC9A765FF4E7974C6127C697422D6663B03E56BD272033E696C). Examples are information gathered after IDA data cleaning, exclusion of cases, unusual observation, creating of new or transformed variables, distributions, or missing data patterns [[3]](https://obsstudies.org/wp-content/uploads/2018/04/idarev2.pdf). \
In a conceptual framework, IDA can be thought as a process of data inspection divided into some steps, where some authors divide into three, while others divide into six steps. Here we will show the *six steps* path as shown in [[3]](https://obsstudies.org/wp-content/uploads/2018/04/idarev2.pdf), as in the Figure above:

![image](https://user-images.githubusercontent.com/63553829/94266078-fa784600-ff0f-11ea-896f-222cfc91b213.png){: .align-center}

#### 1.1. Metadata setup

Sometimes the dataset alone isn't sufficient to do a meaningful analysis, so background information is needed, or, in other words, *metadata* - "structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use, or manage an information". Here we have **technical metadata**, such as variables and values, data type, measurement unit, plausability limits or codes for missing values, essentail for IDA data cleaning and data screening; and **conceptual data**, which relates to the background of data collection and its implementation, regarding design (data sources and collection methods), recruitment (target population, sampling methods, time of data collection), or study (validation status of instruments, training of examiners).\
Metadata of relevance msut be added in a systematic way so that one can cover all elements of interest. A good practice is to set up all relevant metadata into an electronic form such as databases (e.g. MySQL) before starting the study.

#### 1.2. Data cleaning

As the name says, here data is processed in order to arrive at a cleaned dataset in the needed format for the statistical analysis. This may include several different steps such as creating of new variables, splitting or merging datasets, and must be performed in a systematic way to find data errors (which are highly variable) and, if possible, correct them.\
This step can be performed by inspection with the use of simple techniques such as tabulations, cross tabulations, histograms, or scatter plots. some best practices are using well established coding schemes or formats such as for date and time.

#### 1.3. Data screening

Data screening tryies to understand properties of data that may affect future analysis, or, in other words, is an attempt to detect structural errors which points  to several different problems (e.g. recruitment process, choice of instruments, etc.), which result in a deviation between the expected and observed data. A simple example consists in imbalanced data, so let's imagine one wants to assess risk level. In this case one must include subjects with low, medium, and high risk profile. But if for some reason the number of subjects on high risk are rare (extremely low number of subjects regarding the others - in-class imbalance), this study would result in only limited conclusions. The aim here is hence variable, so let's brief it:

- *distribution of single variables:* identify qualitative properties (outliers, bimodality, skewness) and compare characteristics through visual inspection (boxplots,histograms, bar charts, etc.)
- *missing data:* methods to handle missing data, with their types, level of occurence, etc.
- *association between variables:* comparison, visualization and quantification of pairwise associations, and developing ideas for summary variables to reduce dimensionality

#### 1.4. Documentation and initial data reporting

The previous steps are laborious and a large amount of information is created. So in order to avoid losing information over these steps, an initial data report is quite useful and necessary. Here we have another briefing of what should be put in that report:
1. A short summary of the metadata set up step, focusing on the sources of metadata, and a summary of the research questions and study design.
2. A detailed study flowchart, starting with all criteria defining population and recruitment process, number of subjects before and after inclusion/exclusion criteria, impossible values, missing data, etc.
3. A short summary of data processing and cleaning step, with all correction rules and impact of changes (e.g. proportion of elements when dealing with imbalanced data).
4. A short summary with data screening with description of variables' distributions, and in some cases further tables with missing values frequencies, multivariable descriptions, etc.
5. A summary with all insights which can influence how the results are intepreted, including deviation between expected and observed values.
6. A summary with all insights which can influence further statistical analysis, which are data properties that are not in accord with requirements of the statistical methods.

#### 1.5. Refining and updating the statistical analysis plan

A key aspect of IDA is to avoid misleading statistical analysis due to incomprehension about data properties. So after data processing, cleaning and screening, and reported, it may be needed that the statistical analysis plan is refined, updated, or extended. As an example, let's see some issues, examples, and possible influence over the statistical analysis:
| **Issue** | **Examples** | **Possible influence** |
|:----|:----|:----|:----|
| Suspicious values | Outliers, or inconsistent floow-up dates | Decision of whether to use them or not in the statistical analysis |
| Suspicious subjects/rows | Data looks so corrupt that may add more noise than information | Decision whether to exclude data for balancing |
| Distribution of variable | Skew or bimodal distributions | Transformation of variables, splitting bimodal into binary, or adapt statistical method |
| Data is not in accordance with statistical methods requirements | Outcome variable skewed distribution, imbalanced data | Refinement, extensions or reductions |

#### 1.6. Reporting IDA in research papers

While initial data report include a list of activities and insights, the IDA report is comprehensive and focuses on findings of relevance for the interpretation of results, with changes regarding the analysis and their motivations. Study limitations should be discussed in a section.


As can be seen, IDA is quite laborious. Some people classify IDA as only assessing for missing values, which now can be clearly viewed as an oversimplification of a comprehensive process. Now let's quickly check EDA, and see their similarities.


## 2. EDA

*Exploratory Data Analysis (EDA)* can be thought as a good practice for data analysis that employed a variety of techniques, which are most graphical, in order to maximize insights into a dataset, detect mistakes, uncovering underlyig structures such as relationships among explanatory variables, extract important variables, detecting outliers and anomalies, testing underlying assumptions, develop lean models, among others [[4]](https://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm). Summarizing, EDA is a philosophy as to how dissect a dataset, about what we look for, how we look, and how we interpret it. So it's important to distinguish that EDA is not a set of techniques (statistical graphs), per se, but makes use of those statistical graphs as means to an end.\
EDA is usually cross-classified in two ways, where the first is a non-graphical or graphical method, and the second, either univariate or multivariate. Although there are some  guidelines, EDA shouldn't be fix, since sometimes a look from a different angle can result in greater insights. Here we'll divide EDA in to four types: univariate non-graphical, multivariate non-graphical, univariate graphical, and multivariate graphical, as presented in [[5]](http://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf). So let's begin!

#### 2.1. Univariate non-graphical









